{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmjJ5sZ3lXLNlMqnl0hB/k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bphqk3/CF/blob/main/CF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This aims to cluster only active players in the 2023-2024 season. Players will be dropped entirely in previous seasons if they're not in the league anymore**"
      ],
      "metadata": {
        "id": "QvUltH6WgAOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Fri Apr  5 01:34:00 2024\n",
        "\n",
        "@author: Brooke\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import time\n",
        "from io import StringIO\n",
        "import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit\n",
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "from sklearn.linear_model import LinearRegression, LassoCV, Lasso, RidgeCV, Ridge, ElasticNetCV, ElasticNet, BayesianRidge, LogisticRegression, SGDRegressor, RidgeClassifier\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score\n",
        "from sklearn.experimental import enable_hist_gradient_boosting  # Enable the module\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
        "import pickle\n",
        "from scipy.stats import pearsonr\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
        "import plotly.graph_objects as go"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDT1QRTGiLAH",
        "outputId": "63dc12b0-b3bb-4d67-e5da-304b44d09f38"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/experimental/enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# make consolidated season stats list\n",
        "\n",
        "*   structures are different in each. Manually drop irrelevant or duplicated columns before merging per the stat in interest (PTS). Add prefixes where necessary\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5wHg7ImfhAY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = pd.read_parquet('/home/S2122/player_stat_history_traditional2122.parquet')\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "lTLCOmZClyQG",
        "outputId": "ddbf3cfb-d291-40ff-d59b-063d468a2a19"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-15-bce4f1c105f8>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-bce4f1c105f8>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    git add README.md\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = a.drop(['Name', 'Unnamed: 0', 'Team','GP', 'W', 'L','FGM','3PM','FTM','FP',\n",
        "            'FGM RANK', 'FGA RANK', 'FG% RANK', '3PM RANK', '3PA RANK', '3P% RANK',\n",
        "            'FTM RANK', 'FTA RANK', 'FT% RANK', 'OREB RANK', 'DREB RANK',\n",
        "            'REB RANK', 'AST RANK', 'TOV RANK', 'STL RANK', 'BLK RANK', 'PF RANK',\n",
        "            'FP RANK', 'DD2 RANK', 'TD3 RANK', '+/- RANK','+/-', 'GP RANK', 'W RANK', 'L RANK',\n",
        "            'MIN RANK', 'PTS RANK'],axis = 1)\n",
        "\n",
        "b = pd.read_parquet(r'C:/Users/Brooke/Desktop/rbt/rbt_player_history/nbacom/season_history/S2122/player_stat_history_drives2122.parquet')\n",
        "b"
      ],
      "metadata": {
        "id": "jf5Eya7wv489"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b = b.drop(['Name', 'Team', 'GP', 'W', 'L', 'MIN','FGM', 'FGA',\n",
        "       'FG%', 'FTM', 'FTA', 'FT%', 'PTS', 'PTS%', 'PASS', 'PASS%', 'AST',\n",
        "       'AST%', 'TO', 'TOV%', 'PF', 'PF%'], axis = 1)\n",
        "b.rename(columns={'DRIVES':'DrivesPG'}, inplace = True)\n",
        "\n",
        "#got rid of \"misc\" stats (c)\n",
        "\n",
        "#got rid of \"passing\" stats (d)\n",
        "\n",
        "e = pd.read_parquet(r'C:/Users/Brooke/Desktop/rbt/rbt_player_history/nbacom/season_history/S2122/player_stat_history_rebounding2122.parquet')\n",
        "e"
      ],
      "metadata": {
        "id": "F6hLGlprwald"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "e = e.drop(columns=['Name', 'Team', 'GP', 'W', 'L', 'MIN', 'REB', 'Contested REB',\n",
        "       'Contested REB%', 'REB Chances'], axis = 1)\n",
        "e = e.iloc[:, :-3]\n",
        "e.rename(columns={'REB Chance%' :'REB Chance% PG'}, inplace = True)\n",
        "\n",
        "f = pd.read_parquet(r'C:/Users/Brooke/Desktop/rbt/rbt_player_history/nbacom/season_history/S2122/player_stat_history_scoring2122.parquet')\n",
        "f"
      ],
      "metadata": {
        "id": "K6NAcQswwjgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = f.drop(columns=['Name', 'Unnamed: 0', 'Team', 'AGE', 'GP', 'W', 'L', 'MIN',\n",
        "       '%FGA 2PT', '%FGA 3PT',\n",
        "       '%PTS FBPs', '%PTS OFFTO', '%PTS PITP', '2FGM %AST',\n",
        "       '2FGM %UAST', '3FGM %AST', '3FGM %UAST', 'FGM %AST', 'FGM %UAST',\n",
        "       'GP RANK', 'W RANK', 'L RANK', 'MIN RANK', '%FGA 2PT RANK',\n",
        "       '%FGA 3PT RANK', '%PTS 2PT RANK','%PTS 3PT RANK',\n",
        "       '%PTS FBPs RANK', '%PTS FT RANK', '%PTS OffTO RANK', '%PTS PITP RANK',\n",
        "       '2FGM %AST RANK', '2FGM %UAST RANK', '3FGM %AST RANK',\n",
        "       '3FGM %UAST RANK', 'FGM %AST RANK', 'FGM %UAST RANK'],axis = 1)\n",
        "idrop = [2,5]\n",
        "f = f.drop(f.columns[idrop], axis=1)\n",
        "f.rename(columns={'%PTS 2PT':'%PTS 2PT PG', '%PTS 3PT':'%PTS 3PT PG','%PTS FT':'%PTS FT PG'}, inplace = True)\n",
        "\n",
        "g = pd.read_parquet(r'C:/Users/Brooke/Desktop/rbt/rbt_player_history/nbacom/season_history/S2122/player_stat_history_shooting2122.parquet')\n",
        "g\n"
      ],
      "metadata": {
        "id": "5dHPD0xLwpET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g = g.drop(columns=['Name', 'Team', 'Age', 'RA_FGM','PiPnRA_FGM','MR_FGM','LC3_FGM','RC3_FGM','C3_FGM','AB3_FGM'],axis=1)\n",
        "suffix = ' PG'\n",
        "g.columns.values[1:] = [col + suffix for col in g.columns[1:]]\n",
        "g = g.replace('-', 0)\n",
        "g.iloc[:, 1:] = g.iloc[:, 1:].apply(pd.to_numeric)\n",
        "\n",
        "\n",
        "\n",
        "#we can drop (i)\n",
        "j = pd.read_parquet(r'C:/Users/Brooke/Desktop/rbt/rbt_player_history/nbacom/season_history/S2122/player_stat_history_advanced2122.parquet')\n",
        "j"
      ],
      "metadata": {
        "id": "WsizPKdQwt5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "j = j.drop(['Name', 'Unnamed: 0', 'Team', 'AGE', 'GP', 'W', 'L', 'MIN', 'AST/TO','POSS', 'FGM',\n",
        "        'FGA', 'FG%', 'GP RANK', 'W RANK','L RANK', 'MIN RANK', 'OFFRTG RANK', 'DEFRTG RANK', 'NETRTG RANK',\n",
        "        'AST% RANK', 'AST/TO RANK', 'OREB% RANK',\n",
        "        'DREB% RANK', 'REB% RANK', 'eFG% RANK', 'TS% RANK',\n",
        "        'USG% RANK', 'PACE RANK', 'PIE RANK', 'FGM RANK',\n",
        "        'FG% RANK','OREB%','DREB%',],axis=1)\n",
        "j = j.drop(j.columns[7], axis=1).iloc[:,:12]\n",
        "\n",
        "h = pd.read_parquet(r'C:/Users/Brooke/Desktop/rbt/rbt_player_history/nbacom/season_history/S2122/player_stat_history_clutch-usage2122.parquet')\n",
        "h"
      ],
      "metadata": {
        "id": "L8ybr2GUww9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h = h.drop(['Name', 'Unnamed: 0', 'Team', 'AGE', 'GP', 'W', 'L', '%FGM', '%FGA', '%3PM', '%3PA', '%FTM', 'GP RANK', 'W RANK', 'L RANK', 'MIN RANK',\n",
        "          '%FTA', '%OREB','%DREB', '%TOV', '%STL', '%BLK', '%BLKA', '%PF', '%PFD','GP RANK', 'W RANK',\n",
        "            'L RANK', 'MIN RANK', 'USG% RANK','%FGM RANK', '%FGA RANK', '%3PM RANK', '%3PA RANK', '%FTM RANK',\n",
        "            '%FTA RANK', '%OREB RANK', '%DREB RANK', '%REB RANK', '%AST RANK','%TOV RANK',\n",
        "            '%STL RANK', '%BLK RANK', '%BLKA RANK', '%PF RANK','%PFD RANK', '%PTS RANK','MIN'],axis = 1)\n",
        "\n",
        "#capture the players that are clutch; fill non-clutch players with zeros for their stats when we merge\n",
        "h['PLAYER'] = 1\n",
        "new_names = [(i,'Clutch_Game_'+i) for i in h.iloc[:, 1:].columns.values]\n",
        "h.rename(columns = dict(new_names), inplace=True)\n",
        "\n",
        "k = pd.read_parquet(r'C:/Users/Brooke/Desktop/rbt/rbt_player_history/nbacom/season_history/S2122/player_stat_history_clutch-advanced2122.parquet')\n",
        "k"
      ],
      "metadata": {
        "id": "HUctoBgjwyh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = k.drop(['Name', 'Unnamed: 0', 'Team', 'AGE', 'GP', 'W', 'L', 'MIN', 'AST/TO', 'FGM',\n",
        "            'OffRtg', 'DefRtg', 'NetRtg', 'AST%', 'REB%','eFG%',\n",
        "            'FGA', 'FG%', 'GP RANK', 'W RANK','L RANK', 'MIN RANK',\n",
        "            'AST% RANK', 'AST/TO RANK', 'OREB% RANK',\n",
        "            'DREB% RANK', 'REB% RANK', 'eFG% RANK', 'TS% RANK',\n",
        "            'USG% RANK', 'PACE RANK', 'PIE RANK', 'FGM RANK',\n",
        "            'FG% RANK','OREB%','DREB%',],axis=1)\n",
        "kdrop = [1,2]\n",
        "k = k.drop(k.columns[kdrop], axis=1).iloc[:,:5]\n",
        "new_names = [(i,'Clutch_Game_'+i) for i in k.iloc[:, 1:].columns.values]\n",
        "k.rename(columns = dict(new_names), inplace=True)\n",
        "\n",
        "l = pd.read_parquet(r'C:/Users/Brooke/Desktop/rbt/rbt_player_history/nbacom/season_history/S2122/player_stat_history_clutch-traditional2122.parquet')\n",
        "l"
      ],
      "metadata": {
        "id": "6snGsUt9w0NT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l = l.drop(['Name', 'Unnamed: 0', 'Team','GP', 'W', 'L','FGM','3PM','FTM','FP',\n",
        "            'FGM RANK', 'FGA RANK', 'FG% RANK', '3PM RANK', '3PA RANK', '3P% RANK',\n",
        "            'FTM RANK', 'FTA RANK', 'FT% RANK', 'OREB RANK', 'DREB RANK',\n",
        "            'REB RANK', 'AST RANK', 'TOV RANK', 'STL RANK', 'BLK RANK', 'PF RANK',\n",
        "            'DD2 RANK', 'TD3 RANK', '+/- RANK','+/-', 'GP RANK', 'W RANK', 'L RANK',\n",
        "            'MIN RANK', 'PTS RANK','Age'],axis = 1)\n",
        "new_names = [(i,'Clutch_Game_'+i) for i in l.iloc[:, 1:].columns.values]\n",
        "l.rename(columns = dict(new_names), inplace=True)\n",
        "\n",
        "S2122_Player_Stats = pd.merge(a, j, on='Player', how='outer').merge(e, on='Player', how='outer').merge(f, on='Player', how='outer').merge(g, on='Player', how='outer').merge(b, on='Player', how='outer').merge(h, on='Player', how='outer').merge(k, on='Player', how='outer').merge(l, on='Player', how='outer').fillna(0)\n",
        "\n",
        "S2122_Player_Stats_noClutch = pd.merge(a, j, on='Player', how='outer').merge(e, on='Player', how='outer').merge(f, on='Player', how='outer').merge(g, on='Player', how='outer').merge(b, on='Player', how='outer').fillna(0)\n"
      ],
      "metadata": {
        "id": "JfndQRn0w1ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Player Names that meet the criteria to be analyzed (4/5/2024)\n",
        "\n",
        "\n",
        "*   Filter players 2023-2024 season by avg min > 15 and number games played > 20\n",
        "*   copy stats to \"data\" df with the 'Player' column removed so only numeric values exist\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0-NE5XY-i3fv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "S2324 = pd.read_parquet(r'C:\\Users\\Brooke\\Desktop\\rbt\\rbt_player_history\\player_full_stat_history.parquet')\n",
        "S2324 = S2324.groupby('Player').filter(lambda x: x['MIN'].mean() > 15 and len(x) >= 20)\n",
        "S2324_Player_Names = S2324['Player'].unique().tolist()\n",
        "\n",
        "Players_2122 = S2122_Player_Stats_noClutch[S2122_Player_Stats_noClutch['Player'].isin(S2324_Player_Names)]\n",
        "Players_2122 = Players_2122.copy()\n",
        "Players_2122.sort_values(by=['Player'], inplace = True)\n",
        "Players_2122.reset_index(drop=True,inplace = True)\n",
        "\n",
        "data = Players_2122.copy()\n",
        "data = data.drop(['Player'],axis=1)\n",
        "data_col_names = data.columns"
      ],
      "metadata": {
        "id": "fEuiSinKin8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Correlation"
      ],
      "metadata": {
        "id": "PiegUi89k6uH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the correlation matrix\n",
        "cor_matrix = data.corr()\n",
        "\n",
        "# Plot correlation heatmap using seaborn\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cor_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation of NBA Stats')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Filter for positive correlations and exclude diagonal\n",
        "positive_corr = cor_matrix.mask(np.tril(np.ones(cor_matrix.shape, dtype=bool))).stack().sort_values(ascending=False)\n",
        "\n",
        "# Filter for significant correlations (greater than 0.9)\n",
        "significant_corr = positive_corr[positive_corr > 0.9]\n",
        "\n",
        "# Get the first 10 pairs of positively correlated variables\n",
        "top_corr = significant_corr\n",
        "# Plot the cross-correlation\n",
        "plt.figure(figsize=(10, 6))\n",
        "top_corr.plot(kind='bar', color='skyblue')\n",
        "plt.title('Top 10 Positively Correlated Variable Pairs')\n",
        "plt.xlabel('Variable Pair')\n",
        "plt.ylabel('Correlation Coefficient')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "significant_corr_pairs = []\n",
        "for var1 in data.columns:\n",
        "    for var2 in data.columns:\n",
        "        if var1 < var2:\n",
        "            corr, p_value = pearsonr(data[var1], data[var2])\n",
        "            if np.abs(corr) > 0.5 and p_value < 0.05:  # Set the significance level at 5%\n",
        "                significant_corr_pairs.append((var1, var2))\n",
        "\n",
        "print(significant_corr_pairs[:10])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jfCXQxVokBjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is performing cross-correlation analysis on pairs of variables in a dataset to identify significant correlations based on the Pearson\n",
        "correlation coefficient and p-value. Here's a breakdown of what the code is doing:\n",
        "\n",
        "Iterating Over Columns: The code iterates over each\n",
        "column (variable) in the dataset twice using nested loops. This way, it considers all possible pairs of variables.\n",
        "\n",
        "Calculating Correlation: For each pair of variables (var1, var2), it calculates the Pearson correlation coefficient (corr) and the p-value (p_value) using the pearsonr function from the scipy.stats module.\n",
        "\n",
        "Checking Significance: It checks if the absolute value of the correlation coefficient is greater than 0.5 and if the p-value is less than 0.05 (significance level set at 5%). If both conditions are met, the pair (var1, var2) is considered a significant correlation pair.\n",
        "\n",
        "Storing Significant Pairs: The significant correlation pairs are stored in the significant_corr_pairs list.\n",
        "\n",
        "Displaying Results: The first 10 significant correlation pairs are printed, and then plt.show() is called to display any plots (though there are no plots generated in the provided code).\n",
        "\n",
        "A significant correlation means that there is a statistically meaningful relationship between two variables. In the context of the provided code, significance is determined based on two criteria:\n",
        "\n",
        "Correlation Coefficient: The absolute value of the Pearson correlation coefficient (corr) must be greater than 0.5. This indicates a strong linear relationship between the variables.\n",
        "\n",
        "P-value: The p-value (p_value) must be less than 0.05, which is a common significance level used in statistical hypothesis testing. A p-value below this threshold suggests that the observed correlation coefficient is unlikely to have occurred by random chance alone.\n",
        "\n",
        "Keeping both pairs of variables in your dataset depends on the specific context of your analysis and your goals. Here are some considerations:\n",
        "\n",
        "Relevance: Are the variables meaningful and relevant to your analysis? If the variables are not relevant to your research question or analysis objectives, it may be unnecessary to keep them in the dataset.\n",
        "\n",
        "Collinearity: If two variables are highly correlated, they may provide redundant information. In some cases, multicollinearity (high correlation between predictors) can cause issues in statistical modeling, such as inflated standard errors or unstable coefficient estimates. In such cases, you may choose to keep only one of the correlated variables or consider techniques like principal component analysis (PCA) to reduce dimensionality.\n",
        "\n",
        "Interpretability: Consider whether the correlation between the variables makes sense in the context of your data and domain knowledge. Sometimes, statistically significant correlations may not have a clear causal explanation or practical significance.\n",
        "\n",
        "Data Quality: Ensure that the observed correlations are not artifacts of data quality issues, such as outliers or measurement errors. It's important to carefully examine the data and consider potential confounding variables.\n",
        "\n",
        "Ultimately, the decision to keep or remove correlated pairs of variables should be guided by your research objectives, domain knowledge, and the specific requirements of your analysis. If in doubt, it's often helpful to consult with domain experts or statisticians for guidance.\n",
        "\n",
        "##Good news! I tried signifance levels of 1%, 10%, and 30% and calculated the same number of signficant corr pairs! Per chatgpt: Overall,observing the same number of significant correlations across different significance levels suggests that the relationships between variables are likely to be meaningful and robust. This suggests that the observed correlations are likely to be genuine and not simply due to random chance.However, it's important to interpret the results in conjunction with other factors and consider the implications for the research question and subsequent analyses.\n"
      ],
      "metadata": {
        "id": "Xj-oTH7UmOa_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Boxplot Analysis"
      ],
      "metadata": {
        "id": "aTWcebXym6KW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_cols = data.shape[1]\n",
        "num_rows = 10\n",
        "num_cols_per_row = 5\n",
        "\n",
        "# Create a figure and axes\n",
        "fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols_per_row, figsize=(20, 30))\n",
        "\n",
        "# Flatten the axes array to simplify iteration\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Iterate over columns and create boxplots\n",
        "for i, col in enumerate(data.columns):\n",
        "    ax = axes[i]\n",
        "    ax.boxplot(data[col])\n",
        "    ax.set_title(col)\n",
        "    ax.grid(True)\n",
        "\n",
        "# Hide any empty subplots\n",
        "for i in range(num_cols, num_rows * num_cols_per_row):\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "data = scaler.fit_transform(data)\n",
        "\n",
        "## after this scaling, the df becomes an array! ##\n",
        "\n",
        "axes = axes.flatten()\n",
        "\n",
        "fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols_per_row, figsize=(20, 4 * num_rows))\n",
        "\n",
        "# Flatten the axes array to simplify iteration\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Iterate over columns and create boxplots\n",
        "for i in range(num_cols):\n",
        "    ax = axes[i]\n",
        "    ax.boxplot(data[:, i])\n",
        "    ax.set_title(f'Column {i+1}')\n",
        "    ax.grid(True)\n",
        "\n",
        "# Hide any empty subplots\n",
        "for i in range(num_cols, num_rows * num_cols_per_row):\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mGVfRwNOngf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform Principal Component Analysis (PCA)"
      ],
      "metadata": {
        "id": "T1iIMmrHoC1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA().fit(data)\n",
        "#Plotting the Cumulative Summation of the Explained Variance\n",
        "plt.figure()\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Variance (%)') #for each component\n",
        "plt.title('Explained Variance')\n",
        "plt.show()\n",
        "#n = 24 looks like it capture ~95+% variance\n",
        "#n = 15 captured 90% of the variance from Osken/Onay paper with similar stats...\n",
        "\n",
        "pca = PCA(n_components=24)\n",
        "data_pca = pca.fit_transform(data)\n",
        "with open('data_pca_n24.pkl', 'wb') as f:\n",
        "    pickle.dump(data_pca, f)\n",
        "\n",
        "#'pca' is dtype = decomposition\n",
        "#'data_pca' is array with dtype float 64\n",
        "components = pca.components_\n",
        "\n",
        "num_pcs = 24\n",
        "num_vars = components.shape[1]\n",
        "num_rows = 6\n",
        "\n",
        "plt.figure(figsize=(24,24))  # Adjust the figure size as needed\n",
        "\n",
        "for i in range(num_pcs):\n",
        "    plt.subplot(num_rows, num_pcs // num_rows, i+1)  # Create subplots in 6 rows\n",
        "    plt.bar(range(num_vars), components[i])\n",
        "    plt.xticks(range(num_vars), data_col_names,rotation=90)  # Rotate x-axis labels for better readability\n",
        "    plt.title(f'Contribution of variables to PC{i+1}')\n",
        "\n",
        "plt.suptitle('Contribution of NBA stats variables to the principal components')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UvosFqemoHIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement k-means clustering algorithm with PCA data\n",
        "\n"
      ],
      "metadata": {
        "id": "UlSmo3c2onYR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We run our k-means algorithm for all values of k between 5 and 35.\n",
        "To account for the potential impact of random centroid seeds on the cluster\n",
        "robustness, we run each clustering instance of the algorithm 10 times, with\n",
        "different random seeds, selecting the best performing clustering output,\n",
        "using Python’s scikit learn library. For each of these clustering outputs,\n",
        "we estimate the cluster validity using Silhouette value and Calinski-Harabasz\n",
        "pseudo-F index for each run of the algorithm (Chan et al., 2012). Using these\n",
        "two validity indices, we identify best performing k values as k = 25 and k = 26.\n",
        "We also recognise that Bezdek and Bezdek (1981) shows that no cluster validity\n",
        "measure is able to capture the correct number of underlying clusters\n",
        "in every case. Therefore, we also randomly sample other k values to use as\n",
        "inputs for our predictors. We find that as the k value deviates\n",
        "from the 25–26 range in either direction, the prediction success\n",
        "drops."
      ],
      "metadata": {
        "id": "Fi4nTrHPo7_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the range of k values\n",
        "k_values = range(2, 40)\n",
        "\n",
        "# Dictionary to store the evaluation metrics for each k\n",
        "#The average silhouette score is commonly used to evaluate\n",
        "#the overall quality of clustering across different numbers of clusters.\n",
        "#can run other methods take a look at elbow method and intercluster distance maps by yellowbrick\n",
        "silhouette_scores_data_pca = {}\n",
        "avg_ss_data_pca = {}\n",
        "calinski_scores_data_pca = {}\n",
        "\n",
        "# Run k-means clustering 10 times for each k\n",
        "for k in k_values:\n",
        "    silhouette_scores_data_pca[k] = []\n",
        "    avg_ss_data_pca[k]=[]\n",
        "    calinski_scores_data_pca[k] = []\n",
        "    for i in range(10):\n",
        "        #Increasing n_init may provide a more stable and reliable solution\n",
        "        #by exploring a larger range of initializations. It can help\n",
        "        #mitigate the risk of getting stuck in a local minimum and improve the\n",
        "        #chance of finding a solution closer to the global minimum of inertia.\n",
        "        #n_init defaults to 10, higher numbers will cost a lot more...\n",
        "        #50 is considered relatively large. try it and see what resources stand or not\n",
        "        # Random state = None to ensure different seeds; centroids will be initialized\n",
        "        #randomly each time you run the algorithm. (exploratory)\n",
        "        # Random state = integer value to ensure different seeds; centroids will be initialized\n",
        "        #randomly each time you run the algorithm. (need reproducability)\n",
        "        kmeans = KMeans(n_clusters=k, n_init=10, random_state=None)\n",
        "        labels = kmeans.fit_predict(data_pca)\n",
        "        # Calculate silhouette score\n",
        "        silhouette = silhouette_score(data_pca, labels)\n",
        "        silhouette_scores_data_pca[k].append(silhouette)\n",
        "        avg_ss = np.mean(silhouette)\n",
        "        avg_ss_data_pca[k].append(avg_ss)\n",
        "        # Calculate Calinski-Harabasz score\n",
        "        calinski = calinski_harabasz_score(data_pca, labels)\n",
        "        calinski_scores_data_pca[k].append(calinski)\n",
        "\n",
        "# Select the best performing clustering output for each k\n",
        "best_silhouette_data_pca = {k: max(silhouette_scores_data_pca[k]) for k in silhouette_scores_data_pca}\n",
        "best_avg_ss_data_pca = {k: max(avg_ss_data_pca[k]) for k in avg_ss_data_pca[k]}\n",
        "best_calinski_data_pca = {k: max(calinski_scores_data_pca[k]) for k in calinski_scores_data_pca}\n",
        "\n",
        "# Print the best silhouette and Calinski-Harabasz scores for each k\n",
        "print(\"Best Silhouette Scores:\")\n",
        "for k, score in best_silhouette_data_pca.items():\n",
        "    print(f\"k={k}: {score:.4f}\")\n",
        "\n",
        "print(\"Best Average Silhouette Scores:\")\n",
        "for k, score in best_avg_ss_data_pca.items():\n",
        "    print(f\"k={k}: {score:.4f}\")\n",
        "\n",
        "print(\"\\nBest Calinski-Harabasz Scores:\")\n",
        "for k, score in best_calinski_data_pca.items():\n",
        "    print(f\"k={k}: {score:.4f}\")\n",
        "\n",
        "\n",
        "## At this point, you should choose how many clusters you want and you repeat with that n_clusters\n",
        "#consider set random state?\n",
        "k = 8\n",
        "kmeans = KMeans(n_clusters=k, n_init=10, random_state=None)\n",
        "kmeans.fit_predict(data_pca)\n",
        "cluster_labels = kmeans.labels_\n",
        "Players_2122['Cluster FUCKER'] = cluster_labels\n",
        "\n",
        "# Define cluster names\n",
        "cluster_names = {\n",
        "    0: 'Rebounding Bigs',\n",
        "    1: 'Midrange Scorers',\n",
        "    2: 'Three Point Sharpshooters',\n",
        "    3: 'Pass-First Defensive Guards',\n",
        "    4: 'Low Usage Defensive Specialists',\n",
        "    5: 'All-Around Forwards',\n",
        "    6: 'Prehistoric Bigs',\n",
        "    7: 'Ball Dominant Superstars'\n",
        "}\n",
        "Players_2122['Cluster'] = Players_2122['Cluster'].map(cluster_names)\n",
        "\n",
        "# Plotting Clusters\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(x=data_pca[:, 0], y=data_pca[:, 1], hue=Players_2122['Cluster'])\n",
        "plt.title('8 NBA Player Clusters')\n",
        "plt.xlabel('Component 1')\n",
        "plt.ylabel('Component 2')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "km_centers = pd.DataFrame(kmeans.cluster_centers_, columns=data_pca.columns)  # SCALED cluster centers/means\n",
        "\n",
        "# Name clusters before pivoting\n",
        "km_centers['Cluster'] = [f'Cluster {i+1}' for i in range(k)]\n",
        "\n",
        "# Massage data\n",
        "km_centers = km_centers.rename(columns={'astPerMinute': 'AST', 'blkPerMinute': 'BLK',\n",
        "                                        'drbPerMinute': 'DRB', 'fg2aPerMinute': '2PA',\n",
        "                                        'fg3aPerMinute': '3PA', 'ftaPerMinute': 'FTA',\n",
        "                                        'orbPerMinute': 'ORB', 'ptsPerMinute': 'PTS',\n",
        "                                        'stlPerMinute': 'STL', 'tovPerMinute': 'TOV',\n",
        "                                        'pctFT': 'FT%', 'pctFG2': '2P%', 'pctFG3': '3P%',\n",
        "                                        'verticalLeapMaxInches': 'LEAP', 'repsBenchPress135': 'STR',\n",
        "                                        'timeLaneAgility': 'AGL'})\n",
        "\n",
        "km_centers_melted = km_centers.melt(id_vars='Cluster', var_name='feature', value_name='z_val')\n",
        "\n",
        "# Reset the order of predictor variables for plotting\n",
        "predictor_order = ['PTS', 'AST', 'ORB', 'DRB', 'STL', 'BLK', 'TOV', '2PA', '3PA', 'FTA',\n",
        "                   '2P%', '3P%', 'FT%', 'LEAP', 'AGL', 'STR']\n",
        "km_centers_melted['feature'] = pd.Categorical(km_centers_melted['feature'], categories=predictor_order)\n",
        "\n",
        "# Reset the order of clusters for plotting\n",
        "km_centers_melted['Cluster'] = pd.Categorical(km_centers_melted['Cluster'],\n",
        "categories=[f'Cluster {i+1}' for i in range(k)])\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "# Plot cluster 1 # attempting to make a plot like this:https://alexcstern.github.io/hoopDown.html\n",
        "fig = px.scatter(km_centers_melted[km_centers_melted['Cluster'] == 'Cluster 1'],\n",
        "                 x='feature', y='z_val', color='Cluster',\n",
        "                 title='Visualizing K-Means Cluster Makeups - Cluster 1',\n",
        "                 labels={'feature': 'Predictor', 'z_val': 'Cluster Center'},\n",
        "                 color_discrete_map={'Cluster 1': '#232D4B'},\n",
        "                 template='plotly_white')\n",
        "\n",
        "fig.update_layout(xaxis_tickangle=45)\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot cluster centers for each predictor\n",
        "sns.set(style=\"whitegrid\")\n",
        "g = sns.FacetGrid(km_centers, col=\"Cluster\", col_wrap=3, height=4)\n",
        "g.map(sns.scatterplot, \"feature\", \"z_val\", color='blue')\n",
        "\n",
        "# Adjust plot aesthetics\n",
        "g.set_axis_labels(\"Predictor\", \"Cluster Center\")\n",
        "g.set_titles(col_template=\"{col_name}\")\n",
        "g.fig.suptitle(\"Visualizing K-Means Cluster Makeups\", y=1.05)\n",
        "plt.xticks(rotation=90, fontsize=8)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "w-Cs-z10pJa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Hierarchical Clustering"
      ],
      "metadata": {
        "id": "5Tf9hSFioY9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try clustering by single link, centroid, and ward (D2)\n",
        "hcl_single = linkage(y=nba_dist, method='single')\n",
        "hcl_centroid = linkage(y=nba_dist, method='centroid')\n",
        "#this is squared Euclidean distances\n",
        "hcl_ward = linkage(y=nba_dist, method='ward')\n",
        "\n",
        "# This is squared Euclidean distances\n",
        "squared_dissimilarities = pdist(data, metric='sqeuclidean')\n",
        "# Convert the result to a square distance matrix\n",
        "hcl_ward_d2 = linkage(squared_dissimilarities, method='ward')\n",
        "\n",
        "plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
        "plt.plot(hcl_single)\n",
        "plt.title('Single Linkage Dendrogram')\n",
        "plt.xlabel('Distance')\n",
        "plt.ylabel('Data Points')\n",
        "plt.show()\n",
        "\n",
        "# Plot dendrogram for centroid linkage method\n",
        "plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
        "plt.plot(hcl_centroid)\n",
        "plt.title('Link with the Centroid')\n",
        "plt.xlabel('Distance')\n",
        "plt.ylabel('Data Points')\n",
        "plt.show()\n",
        "\n",
        "# Plot dendrogram for Ward\n",
        "plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
        "threshold = 40  # Adjust this threshold value as needed\n",
        "dendrogram(hcl_ward, orientation='top', labels=None, color_threshold=threshold)\n",
        "plt.title('Link with the Ward')\n",
        "plt.xlabel('Distance')\n",
        "plt.ylabel('Data Points')\n",
        "plt.show()\n",
        "\n",
        "# Plot dendrogram for Ward D2\n",
        "plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
        "threshold = 20000  # Adjust this threshold value as needed\n",
        "dendrogram(hcl_ward_d2, orientation='top', labels=None, color_threshold=threshold)\n",
        "plt.title('Link with the Ward Squared Dissimilarities')\n",
        "plt.xlabel('Distance')\n",
        "plt.ylabel('Data Points')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "C_Oe6xiKod0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "u2gJcrRMpSCI"
      }
    }
  ]
}